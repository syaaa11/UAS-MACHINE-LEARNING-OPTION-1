{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTzZVlqjQmXwTEG80NEQac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syaaa11/UAS-MACHINE-LEARNING-OPTION-1/blob/main/Tensor%2C_Autograd%2C_Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Basic"
      ],
      "metadata": {
        "id": "SwrB7zD6NOvn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4spPxkKM_3Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.empty(1)\n",
        "print(x)\n",
        "x = torch.empty(3)\n",
        "print(x)\n",
        "x = torch.empty(2,3)\n",
        "print(x)\n",
        "x = torch.empty(2,2,3)\n",
        "print(x)\n",
        "\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "\n",
        "x = torch.zeros(5, 3)\n",
        "print(x)\n",
        "print(x.size())\n",
        "print(x.dtype)\n",
        "\n",
        "x = torch.zeros(5, 3, dtype=torch.float16)\n",
        "print(x)\n",
        "\n",
        "print(x.dtype)\n",
        "\n",
        "x = torch.tensor([5.5, 3])\n",
        "print(x.size())\n",
        "\n",
        "x = torch.tensor([5.5, 3], requires_grad=True)\n",
        "\n",
        "y = torch.rand(2, 2)\n",
        "x = torch.rand(2, 2)\n",
        "\n",
        "z = x + y\n",
        "\n",
        "z = x - y\n",
        "z = torch.sub(x, y)\n",
        "\n",
        "z = x * y\n",
        "z = torch.mul(x,y)\n",
        "\n",
        "z = x / y\n",
        "z = torch.div(x,y)\n",
        "\n",
        "x = torch.rand(5,3)\n",
        "print(x)\n",
        "print(x[:, 0])\n",
        "print(x[1, :])\n",
        "print(x[1,1])\n",
        "\n",
        "print(x[1,1].item())\n",
        "\n",
        "x = torch.randn(4, 4)\n",
        "y = x.view(16)\n",
        "z = x.view(-1, 8)\n",
        "print(x.size(), y.size(), z.size())\n",
        "\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "print(type(b))\n",
        "\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "import numpy as np\n",
        "a = np.ones(5)\n",
        "b = torch.from_numpy(a)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    y = torch.ones_like(x, device=device)\n",
        "    x = x.to(device)\n",
        "    z = x + y\n",
        "    z.to(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd"
      ],
      "metadata": {
        "id": "tiT0EiHlNxvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x + 2\n",
        "print(x)\n",
        "print(y)\n",
        "print(y.grad_fn)\n",
        "\n",
        "z = y * y * 3\n",
        "print(z)\n",
        "z = z.mean()\n",
        "print(z)\n",
        "\n",
        "z.backward()\n",
        "print(x.grad)\n",
        "\n",
        "# -------------\n",
        "# Model dengan non-scalar output:\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "y = x * 2\n",
        "for _ in range(10):\n",
        "    y = y * 2\n",
        "\n",
        "print(y)\n",
        "print(y.shape)\n",
        "\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
        "y.backward(v)\n",
        "print(x.grad)\n",
        "\n",
        "# -------------\n",
        "# Stop tensor dari tracking history:\n",
        "\n",
        "a = torch.randn(2, 2)\n",
        "print(a.requires_grad)\n",
        "b = ((a * 3) / (a - 1))\n",
        "print(b.grad_fn)\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)\n",
        "\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(a.requires_grad)\n",
        "b = a.detach()\n",
        "print(b.requires_grad)\n",
        "\n",
        "a = torch.randn(2, 2, requires_grad=True)\n",
        "print(a.requires_grad)\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)\n",
        "\n",
        "# -------------\n",
        "# backward() mengakumulasikan gradien untuk tensor ini ke dalam .grad attribute.\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model_output = (weights*3).sum()\n",
        "    model_output.backward()\n",
        "\n",
        "    print(weights.grad)\n",
        "    with torch.no_grad():\n",
        "        weights -= 0.1 * weights.grad\n",
        "    weights.grad.zero_()\n",
        "\n",
        "print(weights)\n",
        "print(model_output)"
      ],
      "metadata": {
        "id": "cMhdY-pCNvx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backpropagation"
      ],
      "metadata": {
        "id": "jD2ZcPWQOhjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "\n",
        "y_predicted = w * x\n",
        "loss = (y_predicted - y)**2\n",
        "print(loss)\n",
        "\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "# melanjutkan optimizing:\n",
        "with torch.no_grad():\n",
        "    w -= 0.01 * w.grad\n",
        "w.grad.zero_()"
      ],
      "metadata": {
        "id": "hP_ednveOiBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**autograd** memungkinkan perhitungan **gradien otomatis** yang diperlukan dalam propagasi mundur untuk memperbarui parameter-parameter jaringan selama pembelajaran mesing (machine learning). Propagasi maju menggunakan **tensor** sebagai input dan melakukan operasi matematika, sementara **propagasi mundur** menggunakan **autograd** untuk menghitung **gradien** dan memperbarui parameter jaringan."
      ],
      "metadata": {
        "id": "KqMX9eI4b16K"
      }
    }
  ]
}