{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPT9aOOdOLYoEDFT3g9og5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syaaa11/UAS-MACHINE-LEARNING-OPTION-1/blob/main/Gradientdescent_manual_and_auto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradientdescent manually"
      ],
      "metadata": {
        "id": "7AQ-eSChfVkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U-iW2O9e_lp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Compute setiap step secara manual\n",
        "\n",
        "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
        "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
        "w = 0.0\n",
        "\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "def gradient(x, y, y_pred):\n",
        "    return np.mean(2*x*(y_pred - y))\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 20\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    y_pred = forward(X)\n",
        "    l = loss(Y, y_pred)\n",
        "    dw = gradient(X, Y, y_pred)\n",
        "    w -= learning_rate * dw\n",
        "    if epoch % 2 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradientdescent automatically"
      ],
      "metadata": {
        "id": "D-clEIimgrSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Di sini kami mengganti gradien yang dihitung secara manual dengan autograd\n",
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "def forward(x):\n",
        "    return w * x\n",
        "\n",
        "def loss(y, y_pred):\n",
        "    return ((y_pred - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "    y_pred = forward(X)\n",
        "    l = loss(Y, y_pred)\n",
        "    l.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "    w.grad.zero_()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')"
      ],
      "metadata": {
        "id": "tisyP5qcgq5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient descent : metode optimisasi yang digunakan dalam machine learning dan deep learning untuk menemukan nilai minimum atau maksimum fungsi objektif. Penting untuk memahami konsep dasar **gradient descent dan menghitung gradien secara manual** agar kita dapat memiliki pemahaman yang mendalam tentang algoritma dan dapat menyesuaikannya jika diperlukan. Tapi, menggunakan **gradient descent automatis** membuat proses pengembangan model lebih mudah dan efisien. Ini juga memungkinkan eksperimen yang lebih cepat dengan variasi model dan algoritma optimisasi. Tidak perlu secara manual menghitung turunan parsial atau mengimplementasikan aturan rantai."
      ],
      "metadata": {
        "id": "ldhME2dPhkvL"
      }
    }
  ]
}